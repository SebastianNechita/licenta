{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The application for pedestrian detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicConfig\n",
      "Possible labels = {'people': (255, 0, 0), 'person-fa': (0, 0, 255), 'person': (0, 255, 0)}\n",
      "Batchs size = 2\n",
      "Steps per epoch = 100\n",
      "Number of epochs = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pedect.config.BasicConfig import BasicConfig\n",
    "config = BasicConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def getVideoPath(self, videoSet, videoNr):\n",
    "        raise Exception(\"Not implemented!\")\n",
    "    \n",
    "    def getAnnotationsPath(self, videoSet, videoNr):\n",
    "        raise Exception(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaltechDataset(Dataset):\n",
    "    def getVideoPath(self, videoSet, videoNr):\n",
    "        return \"..\\\\Data\\\\%s\\\\%s\\\\%s.seq\" % (\"caltech\", videoSet, videoNr)\n",
    "    \n",
    "    def getAnnotationsPath(self, videoSet, videoNr):\n",
    "        return \"..\\\\Data\\\\%s\\\\annotations\\\\%s\\\\%s.vbb\" % (\"caltech\", videoSet, videoNr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenDataset = CaltechDataset()\n",
    "if not isinstance(chosenDataset, Dataset):\n",
    "    raise Exception(\"No such dataset exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video path =  ..\\Data\\caltech\\set00\\V000.seq\n",
      "Annotations path =  ..\\Data\\caltech\\annotations\\set00\\V000.vbb\n"
     ]
    }
   ],
   "source": [
    "import pims\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "video_path = \"\"\n",
    "annotations_path = \"\"\n",
    "\n",
    "\n",
    "if isinstance(chosenDataset, CaltechDataset):\n",
    "    videoSet = \"set00\"\n",
    "    videoNr = \"V000\"\n",
    "    \n",
    "video_path = chosenDataset.getVideoPath(videoSet, videoNr)\n",
    "annotations_path = chosenDataset.getAnnotationsPath(videoSet, videoNr)\n",
    "print(\"Video path = \", video_path)\n",
    "print(\"Annotations path = \", annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import struct\n",
    "# import os\n",
    "# import cPickle\n",
    "# import time\n",
    "from scipy.io import loadmat\n",
    "from collections import defaultdict\n",
    "\n",
    "#Copied from https://github.com/hizhangp/caltech-pedestrian-converter/blob/master/converter.py\n",
    "def read_vbb(path):\n",
    "    assert path[-3:] == 'vbb'\n",
    "    vbb = loadmat(path)\n",
    "    nFrame = int(vbb['A'][0][0][0][0][0])\n",
    "    objLists = vbb['A'][0][0][1][0]\n",
    "    maxObj = int(vbb['A'][0][0][2][0][0])\n",
    "    objInit = vbb['A'][0][0][3][0]\n",
    "    objLbl = [str(v[0]) for v in vbb['A'][0][0][4][0]]\n",
    "    objStr = vbb['A'][0][0][5][0]\n",
    "    objEnd = vbb['A'][0][0][6][0]\n",
    "    objHide = vbb['A'][0][0][7][0]\n",
    "    altered = int(vbb['A'][0][0][8][0][0])\n",
    "    log = vbb['A'][0][0][9][0]\n",
    "    logLen = int(vbb['A'][0][0][10][0][0])\n",
    "\n",
    "    data = {}\n",
    "    data['nFrame'] = nFrame\n",
    "    data['maxObj'] = maxObj\n",
    "    data['log'] = log.tolist()\n",
    "    data['logLen'] = logLen\n",
    "    data['altered'] = altered\n",
    "    data['frames'] = defaultdict(list)\n",
    "\n",
    "    for frame_id, obj in enumerate(objLists):\n",
    "        if len(obj) > 0:\n",
    "            for id, pos, occl, lock, posv in zip(obj['id'][0],\n",
    "                                                 obj['pos'][0],\n",
    "                                                 obj['occl'][0],\n",
    "                                                 obj['lock'][0],\n",
    "                                                 obj['posv'][0]):\n",
    "                keys = obj.dtype.names\n",
    "                id = int(id[0][0]) - 1  # MATLAB is 1-origin\n",
    "                p = pos[0].tolist()\n",
    "                pos = [p[0] - 1, p[1] - 1, p[2], p[3]]  # MATLAB is 1-origin\n",
    "                occl = int(occl[0][0])\n",
    "                lock = int(lock[0][0])\n",
    "                posv = posv[0].tolist()\n",
    "\n",
    "                datum = dict(zip(keys, [id, pos, occl, lock, posv]))\n",
    "                datum['lbl'] = str(objLbl[datum['id']])\n",
    "                # MATLAB is 1-origin\n",
    "                datum['str'] = int(objStr[datum['id']]) - 1\n",
    "                # MATLAB is 1-origin\n",
    "                datum['end'] = int(objEnd[datum['id']]) - 1\n",
    "                datum['hide'] = int(objHide[datum['id']])\n",
    "                datum['init'] = int(objInit[datum['id']])\n",
    "\n",
    "                data['frames'][frame_id].append(datum)\n",
    "\n",
    "    return data\n",
    "annotations = read_vbb(annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnnotationsForFrame(frameNumber, anns = annotations):\n",
    "    temp = anns['frames'][frameNumber]\n",
    "    rez = []\n",
    "    for ann in temp:\n",
    "        rez.append({'pos': ann['pos'], 'lbl': ann['lbl']})\n",
    "    return rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person-fa', 'people', 'person'}\n"
     ]
    }
   ],
   "source": [
    "def getAllLabels(annotations):\n",
    "    allLabels = set()\n",
    "    for frame in range(annotations['nFrame']):\n",
    "        ann = getAnnotationsForFrame(frame)\n",
    "        thisFrameLabels = [ann[i]['lbl'] for i in range(len(ann))]\n",
    "        allLabels = allLabels.union(set(thisFrameLabels))\n",
    "    return allLabels\n",
    "print(getAllLabels(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Format mjpeg detected only with low score of 25, misdetection possible!\n"
     ]
    }
   ],
   "source": [
    "v = pims.PyAVReaderIndexed(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2 as cv\n",
    "from copy import deepcopy\n",
    "\n",
    "def addAnnotationsToImage(img, ann):\n",
    "    img2 = deepcopy(img)\n",
    "    for personAnn in ann:\n",
    "        pos = personAnn['pos']\n",
    "        color = possibleLabels[personAnn['lbl']]\n",
    "        pos = [int(pos[0]), int(pos[1]), int(pos[2]), int(pos[3])]\n",
    "        img2 = cv.rectangle(img2, (pos[0],pos[1]), (pos[0]+pos[2],pos[1]+pos[3]), color, 1)\n",
    "    return img2\n",
    "\n",
    "def getAdnotatedImage(video, frameNumber, annotations = None):\n",
    "    image = video[frameNumber]\n",
    "    if annotations is not None:\n",
    "        ann = getAnnotationsForFrame(frameNumber)\n",
    "        image = addAnnotationsToImage(image, ann)\n",
    "    return image\n",
    "\n",
    "def printImage(video, frameNumber, annotations = None):\n",
    "    image = getAdnotatedImage(video, frameNumber, annotations)\n",
    "    fig, ax = plt.subplots(1, figsize=(20, 10))\n",
    "    ax.imshow(image, animated=True)\n",
    "    plt.show()\n",
    "\n",
    "def saveAdnotatedVideo(video, videoName, annotations = None):\n",
    "    fig, ax = plt.subplots(1, figsize=(20, 10))\n",
    "    frameList = []\n",
    "    for frameNumber in  tqdm_notebook(range(len(video))):\n",
    "        img = getAdnotatedImage(video, frameNumber, annotations)\n",
    "        frameList.append([ax.imshow(img, animated = True)])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, frameList, interval=50, blit=True,\n",
    "                                repeat_delay=10)\n",
    "    print(\"Saving animation...\")\n",
    "    ani.save(videoName)\n",
    "    print(\"Animation saved!\")    \n",
    "\n",
    "# saveAdnotatedVideo(v, \"videoWithAnnotation.mp4\", annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deprecated pixel format used, make sure you did set range correctly\n",
      " (repeated 1844 more times)\n",
      "Format mjpeg detected only with low score of 25, misdetection possible!\n",
      "deprecated pixel format used, make sure you did set range correctly\n"
     ]
    }
   ],
   "source": [
    "def make_generator(images, annotations, batchSize = config.batchSize):\n",
    "    frames = [i for i in range(len(images))]\n",
    "    out_images = []\n",
    "    out_results = []\n",
    "    while True:\n",
    "        np.random.shuffle(frames)\n",
    "        for frameNumber in frames:\n",
    "            c_img = images[frameNumber]\n",
    "            res = 1 if len(getAnnotationsForFrame(frameNumber, annotations)) > 0 else 0\n",
    "            out_images.append(c_img)\n",
    "            out_results.append(res)\n",
    "            if len(out_images) >= batchSize:\n",
    "                yield np.stack(out_images, 0) / 255.0, np.stack(out_results, 0)\n",
    "                out_images, out_results = [], []\n",
    "\n",
    "imageGenerator = make_generator([i for i in v][:100], annotations)\n",
    "x, y = next(imageGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "inputShape = x.shape[1:]\n",
    "print(inputShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 480, 640, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 480, 640, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 240, 320, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 240, 320, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 120, 160, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 120, 160, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 60, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 60, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 15, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1.0, 1.0, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1.0, 1.0, 1)       129       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1.0)               0         \n",
      "=================================================================\n",
      "Total params: 594,049\n",
      "Trainable params: 594,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a = Input(shape = inputShape)\n",
    "b = a\n",
    "decrease = 5\n",
    "if 480 % (2 ** decrease) != 0 or 640 % (2 ** decrease) != 0:\n",
    "    assert(1 == 0)\n",
    "for i in range(decrease):\n",
    "    b = Conv2D(128, (3, 3), activation='relu', padding='same') (b)\n",
    "    b = MaxPooling2D(pool_size = (2, 2))(b)\n",
    "b = MaxPooling2D(pool_size=(480 / (2 ** decrease), 640 / (2 ** decrease)))(b)\n",
    "b = Conv2D(1, (1, 1), activation='sigmoid') (b)\n",
    "b = Flatten()(b)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def tp_rate(y_true, y_pred, smooth = 1e-5):\n",
    "    intersection = K.sum(K.flatten(y_true * y_pred))\n",
    "    rly_true = K.sum(K.flatten(y_true))\n",
    "    return (intersection + smooth) / (rly_true + smooth)\n",
    "def fn_rate(y_true, y_pred, smooth = 1e-5):\n",
    "    return tp_rate(1 - y_true, 1 - y_pred, smooth)\n",
    "\n",
    "model.compile(Adam(), loss='binary_crossentropy', metrics = [tp_rate, fn_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b14c90aca470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstepsPerEpoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrEpochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(imageGenerator, verbose = 2, steps_per_epoch = config.stepsPerEpoch, epochs = config.nrEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez, gt = [], []\n",
    "for i in range(100):\n",
    "    x, y = next(imageGenerator)\n",
    "    rez.append(model.predict(x))\n",
    "    gt.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "a = [1 if i >= 0.5 else 0 for i in [i[0][0] for i in rez] + [i[1][0] for i in rez]]\n",
    "b = [i[0] for i in gt] + [i[1] for i in gt]\n",
    "x = [1 if a[i] == b[i] else 0 for i in range(len(a))]\n",
    "print(x)\n",
    "print(sum(x) / len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
